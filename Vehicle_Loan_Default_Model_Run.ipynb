{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu\\Anaconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from scipy import sparse as ssp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = \"C:\\\\Users\\\\Himanshu\\\\Downloads\\\\train_aox2Jxw\\\\\"\n",
    "train_file = \"train.csv\"\n",
    "test_file =\"test_bqCt9Pv.csv\"\n",
    "\n",
    "train_df = pd.read_csv(Path+train_file)\n",
    "test_df = pd.read_csv(\"C:\\\\Users\\\\Himanshu\\\\Downloads\\\\\"+test_file)\n",
    "\n",
    "# test_df.head()\n",
    "\n",
    "# display(train_df.head())\n",
    "# display(train_df.shape)\n",
    "# display(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    182543\n",
       "1     50611\n",
       "Name: loan_default, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loan_default.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating New Variables\n",
    "\n",
    "\n",
    "def total_months(x):\n",
    "    l =[]\n",
    "    l.append(x.split(' '))\n",
    "    return int(l[0][0][0])*12 + int(l[0][1][0])\n",
    "\n",
    "# 1. Age at loan disbursal date\n",
    "\n",
    "train_df['Date.of.Birth'] = pd.to_datetime(train_df['Date.of.Birth'].values)\n",
    "train_df['DisbursalDate'] = pd.to_datetime(train_df['DisbursalDate'].values)\n",
    "train_df['Age_at_disbursaldate'] = (train_df['DisbursalDate'].dt.date - train_df['Date.of.Birth'].dt.date)\n",
    "train_df['Age_at_disbursaldate'] = train_df['Age_at_disbursaldate'] / np.timedelta64(365, 'D')\n",
    "\n",
    "\n",
    "train_df['Converted_AVERAGE.ACCT.AGE'] = train_df['AVERAGE.ACCT.AGE'].apply(total_months)\n",
    "train_df['Converted_CREDIT.HISTORY.LENGTH'] = train_df['CREDIT.HISTORY.LENGTH'].apply(total_months)\n",
    "\n",
    "\n",
    "\n",
    "df_supplier_id_ftg = pd.DataFrame(train_df.groupby('supplier_id')['loan_default'].apply(lambda x: x.sum()/x.count() if x.count() >=20 else 0)).rename(columns = {'loan_default': 'supplier_id_ftg'})\n",
    "df_branch_id_ftg = pd.DataFrame(train_df.groupby('branch_id')['loan_default'].apply(lambda x: x.sum()/x.count() if x.count() >=20 else 0)).rename(columns = {'loan_default': 'branch_id_ftg'})\n",
    "df_manufacturer_id_ftg = pd.DataFrame(train_df.groupby('manufacturer_id')['loan_default'].apply(lambda x: x.sum()/x.count() if x.count() >=20 else 0)).rename(columns = {'loan_default': 'manufacturer_id_ftg'})\n",
    "df_pincode_id_ftg = pd.DataFrame(train_df.groupby('Current_pincode_ID')['loan_default'].apply(lambda x: x.sum()/x.count() if x.count() >=20  else 0)).rename(columns = {'loan_default': 'pincode_id_ftg'})\n",
    "df_state_id_ftg = pd.DataFrame(train_df.groupby('State_ID')['loan_default'].apply(lambda x: x.sum()/x.count() if x.count() >=20  else 0)).rename(columns = {'loan_default': 'state_id_ftg'})\n",
    "df_employee_code_id_ftg = pd.DataFrame(train_df.groupby('Employee_code_ID')['loan_default'].apply(lambda x: x.sum()/x.count() if x.count() >=20 else 0)).rename(columns = {'loan_default': 'employee_code_id_ftg'})\n",
    "\n",
    "train_df['supplier_id_ftg'] = df_supplier_id_ftg.loc[train_df.supplier_id ,'supplier_id_ftg'].values\n",
    "train_df['branch_id_ftg'] = df_branch_id_ftg.loc[train_df.branch_id ,'branch_id_ftg'].values\n",
    "train_df['manufacturer_id_ftg'] = df_manufacturer_id_ftg.loc[train_df.manufacturer_id ,'manufacturer_id_ftg'].values\n",
    "train_df['pincode_id_ftg'] = df_pincode_id_ftg.loc[train_df.Current_pincode_ID ,'pincode_id_ftg'].values\n",
    "train_df['state_id_ftg'] = df_state_id_ftg.loc[train_df.State_ID ,'state_id_ftg'].values\n",
    "train_df['employee_code_id_ftg'] = df_employee_code_id_ftg.loc[train_df.Employee_code_ID ,'employee_code_id_ftg'].values\n",
    "\n",
    "train_df['Employment_ind'] = train_df['Employment.Type'].apply(lambda x: (1 if x == 'Self employed' else (0 if x == 'Salaried' else -1)))\n",
    "\n",
    "#not using mobile avl flag since it is 1 for every one\n",
    "train_df['Total_verification_flags'] = (train_df['Aadhar_flag'] + train_df['PAN_flag']+ train_df['VoterID_flag'] \\\n",
    "                                        +train_df['Driving_flag'] + train_df['Passport_flag'])\n",
    "\n",
    "\n",
    "train_df.loc[((train_df['VoterID_flag'] == 0) & (train_df['PAN_flag'] == 0)),'VoterId_Pan_flag_Combo'] = 1\n",
    "train_df.loc[((train_df['VoterID_flag'] == 0) & (train_df['PAN_flag'] == 1)),'VoterId_Pan_flag_Combo'] = 2\n",
    "train_df.loc[((train_df['VoterID_flag'] == 1) & (train_df['PAN_flag'] == 0)),'VoterId_Pan_flag_Combo'] = 3\n",
    "train_df.loc[((train_df['VoterID_flag'] == 1) & (train_df['PAN_flag'] == 1)),'VoterId_Pan_flag_Combo'] = 4\n",
    "\n",
    "train_df['Disbursal_Month'] = train_df['DisbursalDate'].dt.month\n",
    "train_df['Disbursal_Day'] = train_df['DisbursalDate'].dt.day\n",
    "\n",
    "\n",
    "test_df['Date.of.Birth'] = pd.to_datetime(test_df['Date.of.Birth'].values)\n",
    "test_df['DisbursalDate'] = pd.to_datetime(test_df['DisbursalDate'].values)\n",
    "test_df['Age_at_disbursaldate'] = (test_df['DisbursalDate'].dt.date - test_df['Date.of.Birth'].dt.date)\n",
    "test_df['Age_at_disbursaldate'] = test_df['Age_at_disbursaldate'] / np.timedelta64(365, 'D')\n",
    "test_df['Age_at_disbursaldate'].head()\n",
    "\n",
    "test_df['Converted_AVERAGE.ACCT.AGE'] = test_df['AVERAGE.ACCT.AGE'].apply(total_months)\n",
    "test_df['Converted_CREDIT.HISTORY.LENGTH'] = test_df['CREDIT.HISTORY.LENGTH'].apply(total_months)\n",
    "\n",
    "\n",
    "test_df['supplier_id_ftg'] = df_supplier_id_ftg.loc[test_df.supplier_id ,'supplier_id_ftg'].values\n",
    "test_df['branch_id_ftg'] = df_branch_id_ftg.loc[test_df.branch_id ,'branch_id_ftg'].values\n",
    "test_df['manufacturer_id_ftg'] = df_manufacturer_id_ftg.loc[test_df.manufacturer_id ,'manufacturer_id_ftg'].values\n",
    "test_df['pincode_id_ftg'] = df_pincode_id_ftg.loc[test_df.Current_pincode_ID ,'pincode_id_ftg'].values\n",
    "test_df['state_id_ftg'] = df_state_id_ftg.loc[test_df.State_ID ,'state_id_ftg'].values\n",
    "test_df['employee_code_id_ftg'] = df_employee_code_id_ftg.loc[test_df.Employee_code_ID ,'employee_code_id_ftg'].values\n",
    "\n",
    "\n",
    "\n",
    "test_df['Total_verification_flags'] = (test_df['Aadhar_flag'] + test_df['PAN_flag']+ test_df['VoterID_flag'] \\\n",
    "                                        +test_df['Driving_flag'] + test_df['Passport_flag'])\n",
    "\n",
    "test_df['Employment_ind'] = test_df['Employment.Type'].apply(lambda x: (1 if x == 'Self employed' else (0 if x == 'Salaried' else -1)))\n",
    "\n",
    "test_df.loc[((test_df['VoterID_flag'] == 0) & (test_df['PAN_flag'] == 0)),'VoterId_Pan_flag_Combo'] = 1\n",
    "test_df.loc[((test_df['VoterID_flag'] == 0) & (test_df['PAN_flag'] == 1)),'VoterId_Pan_flag_Combo'] = 2\n",
    "test_df.loc[((test_df['VoterID_flag'] == 1) & (test_df['PAN_flag'] == 0)),'VoterId_Pan_flag_Combo'] = 3\n",
    "test_df.loc[((test_df['VoterID_flag'] == 1) & (test_df['PAN_flag'] == 1)),'VoterId_Pan_flag_Combo'] = 4\n",
    "\n",
    "test_df['Disbursal_Month'] = test_df['DisbursalDate'].dt.month\n",
    "test_df['Disbursal_Day'] = test_df['DisbursalDate'].dt.day    \n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# pd.crosstab(train_df['Employment.Type'], train_df.loan_default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranformation(data):\n",
    "    \n",
    "    data['PRIMARY.INSTAL.AMT'] = np.log1p(data['PRIMARY.INSTAL.AMT'])\n",
    "    data['SEC.INSTAL.AMT'] = np.log1p(data['SEC.INSTAL.AMT'])\n",
    "    data['SEC.NO.OF.ACCTS'] = np.log1p(data['SEC.NO.OF.ACCTS'])\n",
    "    data['SEC.ACTIVE.ACCTS'] = np.log1p(data['SEC.ACTIVE.ACCTS'])\n",
    "    data['SEC.OVERDUE.ACCTS'] = np.log1p(data['SEC.OVERDUE.ACCTS'])\n",
    "    #data['SEC.CURRENT.BALANCE'] = np.log1p(data['SEC.CURRENT.BALANCE'])\n",
    "    data['SEC.SANCTIONED.AMOUNT'] = np.log1p(data['SEC.SANCTIONED.AMOUNT'])\n",
    "    data['SEC.DISBURSED.AMOUNT'] = np.log1p(data['SEC.DISBURSED.AMOUNT'])\n",
    "\n",
    "    #  filling  missing values in sec.current.balance\n",
    "    data['SEC.CURRENT.BALANCE'].fillna(data['SEC.CURRENT.BALANCE'].mean(), inplace = True)\n",
    "\n",
    "    data['PRI.NO.OF.ACCTS'] = np.log1p(data['PRI.NO.OF.ACCTS'])\n",
    "    data['PRI.ACTIVE.ACCTS'] = np.log1p(data['PRI.ACTIVE.ACCTS'])\n",
    "    data['PRI.OVERDUE.ACCTS'] = np.log1p(data['PRI.OVERDUE.ACCTS'])\n",
    "    #data['PRI.CURRENT.BALANCE'] = np.log1p(data['PRI.CURRENT.BALANCE'])\n",
    "    #data['PRI.SANCTIONED.AMOUNT'] = np.log1p(data['PRI.SANCTIONED.AMOUNT'])\n",
    "    data['PRI.DISBURSED.AMOUNT'] = np.log1p(data['PRI.DISBURSED.AMOUNT'])\n",
    "\n",
    "\n",
    "    #  filling  missing values in sec.current.balance\n",
    "    data['PRI.CURRENT.BALANCE'].fillna(data['PRI.CURRENT.BALANCE'].mean(), inplace = True)\n",
    "    data['PRI.SANCTIONED.AMOUNT'].fillna(data['PRI.SANCTIONED.AMOUNT'].mean(), inplace = True)\n",
    "    data['PERFORM_CNS.SCORE'] = np.log1p(data['PERFORM_CNS.SCORE'])\n",
    "\n",
    "    data['disbursed_amount'] = np.log1p(data['disbursed_amount'])\n",
    "    data['ltv'] = np.log1p(data['ltv'])\n",
    "    data['asset_cost'] = np.log1p(data['asset_cost'])\n",
    "\n",
    "\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('No Bureau History Available', 0)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Sufficient History Not Available', 0)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Not Enough Info available on the customer', 0)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Activity seen on the customer (Inactive)',0)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: No Updates available in last 36 months', 0)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: Only a Guarantor', 0)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('Not Scored: More than 50 active Accounts found',0)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('M-Very High Risk', 1)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('L-Very High Risk', 1)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('K-High Risk', 2)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('J-High Risk', 2)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('I-Medium Risk', 3)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('H-Medium Risk', 3)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('G-Low Risk', 4)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('F-Low Risk', 4)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('E-Low Risk', 4)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('D-Very Low Risk', 5)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('C-Very Low Risk', 5)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('B-Very Low Risk', 5)\n",
    "    data['PERFORM_CNS.SCORE.DESCRIPTION'] = data['PERFORM_CNS.SCORE.DESCRIPTION'].replace('A-Very Low Risk', 5)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tranformation(train_df)\n",
    "test_df = tranformation(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model - 1  (with all the variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 255,  \n",
    "    'max_depth': 12,  \n",
    "    'min_child_samples':50,  \n",
    "    'max_bin': 100,  \n",
    "    'subsample': 0.7,  \n",
    "    'subsample_freq': 1,  \n",
    "    'colsample_bytree': 0.7,  \n",
    "    'min_child_weight': 0,  \n",
    "    'subsample_for_bin': 200,  \n",
    "    'min_split_gain': 0,  \n",
    "    'reg_alpha': 0,  \n",
    "    'reg_lambda': 0,  \n",
    "   # 'nthread': 8,\n",
    "    'verbose': 5,\n",
    "    'scale_pos_weight':99 \n",
    "    }\n",
    "        \n",
    "\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n : 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.762723\tvalid_1's auc: 0.69187\n",
      "[200]\ttraining's auc: 0.812452\tvalid_1's auc: 0.691666\n",
      "Early stopping, best iteration is:\n",
      "[129]\ttraining's auc: 0.779775\tvalid_1's auc: 0.692912\n",
      "time elapsed: 0.43 min\n",
      "fold n : 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.763211\tvalid_1's auc: 0.689123\n",
      "[200]\ttraining's auc: 0.812426\tvalid_1's auc: 0.688239\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's auc: 0.782466\tvalid_1's auc: 0.690618\n",
      "time elapsed: 0.78 min\n",
      "fold n : 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.762987\tvalid_1's auc: 0.69269\n",
      "[200]\ttraining's auc: 0.813827\tvalid_1's auc: 0.694167\n",
      "Early stopping, best iteration is:\n",
      "[140]\ttraining's auc: 0.786015\tvalid_1's auc: 0.694877\n",
      "time elapsed: 1.2  min\n",
      "fold n : 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.76119\tvalid_1's auc: 0.690498\n",
      "[200]\ttraining's auc: 0.811337\tvalid_1's auc: 0.692967\n",
      "Early stopping, best iteration is:\n",
      "[163]\ttraining's auc: 0.795677\tvalid_1's auc: 0.693804\n",
      "time elapsed: 1.7  min\n",
      "fold n : 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.762241\tvalid_1's auc: 0.692253\n",
      "[200]\ttraining's auc: 0.811445\tvalid_1's auc: 0.693376\n",
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's auc: 0.788483\tvalid_1's auc: 0.6942\n",
      "time elapsed: 2.2  min\n",
      "CV score: 0.69324 \n"
     ]
    }
   ],
   "source": [
    "# Doing K fold crossvalidation\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = StratifiedKFold(n_splits= NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "features = ['disbursed_amount', 'asset_cost', 'ltv', 'Aadhar_flag', 'PAN_flag', 'VoterID_flag',\\\n",
    "                'Driving_flag', 'Passport_flag', 'PERFORM_CNS.SCORE', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS', 'PRI.OVERDUE.ACCTS',\n",
    "                'PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT', 'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS',\n",
    "                'SEC.ACTIVE.ACCTS', 'SEC.OVERDUE.ACCTS', 'SEC.CURRENT.BALANCE','employee_code_id_ftg','supplier_id_ftg',\n",
    "               'SEC.SANCTIONED.AMOUNT', 'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT',\n",
    "               'SEC.INSTAL.AMT', 'NEW.ACCTS.IN.LAST.SIX.MONTHS',\n",
    "               'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS', 'NO.OF_INQUIRIES',\n",
    "               'Age_at_disbursaldate',  'branch_id_ftg','pincode_id_ftg',\n",
    "               'manufacturer_id_ftg',  'state_id_ftg', 'Converted_AVERAGE.ACCT.AGE',\n",
    "               'Converted_CREDIT.HISTORY.LENGTH', 'Employment_ind','Total_verification_flags','VoterId_Pan_flag_Combo',\n",
    "           'Disbursal_Month','Disbursal_Day','PERFORM_CNS.SCORE.DESCRIPTION']\n",
    "#  \n",
    "\n",
    "categorical_columns = ['VoterId_Pan_flag_Combo','Disbursal_Month','Employment_ind']\n",
    "predictions1 = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "start_time= time.time()\n",
    "score = [0 for _ in range(folds.n_splits)]\n",
    "oof = np.zeros(len(train_df))\n",
    "max_iter = 5\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, train_df['loan_default'].values)):\n",
    "    print(\"fold n : {}\".format(fold_))\n",
    "    \n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features],\n",
    "                           label=train_df.iloc[trn_idx]['loan_default'], categorical_feature = categorical_columns\n",
    "                          )\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features],\n",
    "                           label=train_df.iloc[val_idx]['loan_default'] , categorical_feature = categorical_columns\n",
    "                          )\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds = 100)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    # we perform predictions by chunks\n",
    "    current_pred = clf.predict(test_df[features].values)\n",
    "    predictions1 += current_pred / min(folds.n_splits, max_iter)\n",
    "   \n",
    "    print(\"time elapsed: {:<5.2}min\".format((time.time() - start_time) / 60))\n",
    "    score[fold_] = roc_auc_score(train_df.iloc[val_idx]['loan_default'].values, oof[val_idx])\n",
    "    if fold_ == max_iter - 1: break\n",
    "        \n",
    "if (folds.n_splits == max_iter):\n",
    "    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(train_df['loan_default'].values, oof)))\n",
    "else:\n",
    "     print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importance_df[feature_importance_df.fold == 5].sort_values(by = 'importance', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model - 2 (excluding the ftg variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n : 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.708791\tvalid_1's auc: 0.640288\n",
      "[200]\ttraining's auc: 0.761265\tvalid_1's auc: 0.638341\n",
      "Early stopping, best iteration is:\n",
      "[122]\ttraining's auc: 0.722742\tvalid_1's auc: 0.640724\n",
      "time elapsed: 0.5  min\n",
      "fold n : 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.708213\tvalid_1's auc: 0.637754\n",
      "[200]\ttraining's auc: 0.760448\tvalid_1's auc: 0.633701\n",
      "Early stopping, best iteration is:\n",
      "[115]\ttraining's auc: 0.718476\tvalid_1's auc: 0.638326\n",
      "time elapsed: 0.98 min\n",
      "fold n : 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.709188\tvalid_1's auc: 0.640178\n",
      "[200]\ttraining's auc: 0.760758\tvalid_1's auc: 0.63569\n",
      "Early stopping, best iteration is:\n",
      "[106]\ttraining's auc: 0.712893\tvalid_1's auc: 0.640624\n",
      "time elapsed: 1.4  min\n",
      "fold n : 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.708363\tvalid_1's auc: 0.641947\n",
      "[200]\ttraining's auc: 0.762194\tvalid_1's auc: 0.640174\n",
      "Early stopping, best iteration is:\n",
      "[134]\ttraining's auc: 0.730899\tvalid_1's auc: 0.642101\n",
      "time elapsed: 1.8  min\n",
      "fold n : 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.708895\tvalid_1's auc: 0.635994\n",
      "[200]\ttraining's auc: 0.761662\tvalid_1's auc: 0.634383\n",
      "Early stopping, best iteration is:\n",
      "[118]\ttraining's auc: 0.719523\tvalid_1's auc: 0.637032\n",
      "time elapsed: 2.3  min\n",
      "CV score: 0.63956 \n"
     ]
    }
   ],
   "source": [
    "# Doing K fold crossvalidation\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = StratifiedKFold(n_splits= NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "features2 = ['disbursed_amount', 'asset_cost', 'ltv', 'Aadhar_flag', 'PAN_flag', 'VoterID_flag',\\\n",
    "                'Driving_flag', 'Passport_flag', 'PERFORM_CNS.SCORE', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS', 'PRI.OVERDUE.ACCTS',\n",
    "                'PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT', 'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS',\n",
    "                'SEC.ACTIVE.ACCTS', 'SEC.OVERDUE.ACCTS', 'SEC.CURRENT.BALANCE',\n",
    "               'SEC.SANCTIONED.AMOUNT', 'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT',\n",
    "               'SEC.INSTAL.AMT', 'NEW.ACCTS.IN.LAST.SIX.MONTHS',\n",
    "               'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS', 'NO.OF_INQUIRIES',\n",
    "               'Age_at_disbursaldate',  'Converted_AVERAGE.ACCT.AGE',\n",
    "               'Converted_CREDIT.HISTORY.LENGTH', 'Employment_ind','Total_verification_flags','VoterId_Pan_flag_Combo',\n",
    "           'Disbursal_Month','Disbursal_Day','PERFORM_CNS.SCORE.DESCRIPTION']\n",
    "#  \n",
    "\n",
    "categorical_columns = ['VoterId_Pan_flag_Combo','Disbursal_Month','Employment_ind']\n",
    "predictions2 = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "start_time= time.time()\n",
    "score = [0 for _ in range(folds.n_splits)]\n",
    "oof = np.zeros(len(train_df))\n",
    "max_iter = 5\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, train_df['loan_default'].values)):\n",
    "    print(\"fold n : {}\".format(fold_))\n",
    "    \n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features2],\n",
    "                           label=train_df.iloc[trn_idx]['loan_default'], categorical_feature = categorical_columns\n",
    "                          )\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features2],\n",
    "                           label=train_df.iloc[val_idx]['loan_default'] , categorical_feature = categorical_columns\n",
    "                          )\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds = 100)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features2], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features2\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    # we perform predictions by chunks\n",
    "    current_pred = clf.predict(test_df[features2].values)\n",
    "    predictions2 += current_pred / min(folds.n_splits, max_iter)\n",
    "   \n",
    "    print(\"time elapsed: {:<5.2}min\".format((time.time() - start_time) / 60))\n",
    "    score[fold_] = roc_auc_score(train_df.iloc[val_idx]['loan_default'].values, oof[val_idx])\n",
    "    if fold_ == max_iter - 1: break\n",
    "        \n",
    "if (folds.n_splits == max_iter):\n",
    "    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(train_df['loan_default'].values, oof)))\n",
    "else:\n",
    "     print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model -3 Only ftg variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n : 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.708783\tvalid_1's auc: 0.641571\n",
      "[200]\ttraining's auc: 0.759974\tvalid_1's auc: 0.638828\n",
      "Early stopping, best iteration is:\n",
      "[125]\ttraining's auc: 0.725054\tvalid_1's auc: 0.642197\n",
      "time elapsed: 0.52 min\n",
      "fold n : 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.707804\tvalid_1's auc: 0.6375\n",
      "[200]\ttraining's auc: 0.759281\tvalid_1's auc: 0.635216\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's auc: 0.712142\tvalid_1's auc: 0.638051\n",
      "time elapsed: 0.91 min\n",
      "fold n : 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.708789\tvalid_1's auc: 0.639016\n",
      "[200]\ttraining's auc: 0.760687\tvalid_1's auc: 0.635934\n",
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's auc: 0.716746\tvalid_1's auc: 0.639638\n",
      "time elapsed: 1.3  min\n",
      "fold n : 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.707045\tvalid_1's auc: 0.640907\n",
      "[200]\ttraining's auc: 0.760945\tvalid_1's auc: 0.639117\n",
      "Early stopping, best iteration is:\n",
      "[131]\ttraining's auc: 0.727596\tvalid_1's auc: 0.641765\n",
      "time elapsed: 1.8  min\n",
      "fold n : 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's auc: 0.708168\tvalid_1's auc: 0.637187\n",
      "[200]\ttraining's auc: 0.760596\tvalid_1's auc: 0.633692\n",
      "Early stopping, best iteration is:\n",
      "[100]\ttraining's auc: 0.708168\tvalid_1's auc: 0.637187\n",
      "time elapsed: 2.2  min\n",
      "CV score: 0.63938 \n"
     ]
    }
   ],
   "source": [
    "# Doing K fold crossvalidation\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = StratifiedKFold(n_splits= NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "features3 = ['employee_code_id_ftg','supplier_id_ftg','branch_id_ftg','pincode_id_ftg',\n",
    "               'manufacturer_id_ftg',  'state_id_ftg']\n",
    "#  \n",
    "\n",
    "categorical_columns = ['VoterId_Pan_flag_Combo','Disbursal_Month','Employment_ind']\n",
    "predictions3 = np.zeros(len(test_df))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "start_time= time.time()\n",
    "score = [0 for _ in range(folds.n_splits)]\n",
    "oof = np.zeros(len(train_df))\n",
    "max_iter = 5\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, train_df['loan_default'].values)):\n",
    "    print(\"fold n : {}\".format(fold_))\n",
    "    \n",
    "    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features2],\n",
    "                           label=train_df.iloc[trn_idx]['loan_default']\n",
    "                          )\n",
    "    val_data = lgb.Dataset(train_df.iloc[val_idx][features2],\n",
    "                           label=train_df.iloc[val_idx]['loan_default'] \n",
    "                          )\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds = 100)\n",
    "    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features2], num_iteration=clf.best_iteration)\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = features3\n",
    "#     fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "#     fold_importance_df[\"fold\"] = fold_ + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    # we perform predictions by chunks\n",
    "    current_pred = clf.predict(test_df[features3].values)\n",
    "    predictions3 += current_pred / min(folds.n_splits, max_iter)\n",
    "   \n",
    "    print(\"time elapsed: {:<5.2}min\".format((time.time() - start_time) / 60))\n",
    "    score[fold_] = roc_auc_score(train_df.iloc[val_idx]['loan_default'].values, oof[val_idx])\n",
    "    if fold_ == max_iter - 1: break\n",
    "        \n",
    "if (folds.n_splits == max_iter):\n",
    "    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(train_df['loan_default'].values, oof)))\n",
    "else:\n",
    "     print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(roc_auc_score(Y_test, final_scores))\n",
    "submission = pd.read_csv(\"C:\\\\Users\\\\Himanshu\\\\Downloads\\\\\"+test_file)\n",
    "submission['loan_default'] = (0.6*predictions1+0.25*predictions2+0.15*predictions3)\n",
    "submission[['UniqueID','loan_default']].to_csv('D2_6th_solution.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRying out different technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['VoterId_Pan_flag_Combo'] = train_df['VoterId_Pan_flag_Combo'].astype('category')\n",
    "train_df['Disbursal_Month'] = train_df['Disbursal_Month'].astype('category')\n",
    "train_df['Employment_ind'] = train_df['Employment_ind'].astype('category')\n",
    "\n",
    "test_df['VoterId_Pan_flag_Combo'] = test_df['VoterId_Pan_flag_Combo'].astype('category')\n",
    "test_df['Disbursal_Month'] = test_df['Disbursal_Month'].astype('category')\n",
    "test_df['Employment_ind'] = test_df['Employment_ind'].astype('category')\n",
    "\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# data['branch_id'] = le.fit_transform(data['branch_id'])\n",
    "# data['manufacturer_id'] = le.fit_transform(data['manufacturer_id'])\n",
    "# data['State_ID'] = le.fit_transform(data['State_ID'])\n",
    "\n",
    "#  ['VoterId_Pan_flag_Combo','Disbursal_Month','Employment_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting dummies for state, supplier id, zip etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233154, 55)\n",
      "(345546, 54)\n"
     ]
    }
   ],
   "source": [
    "test_df['loan_default'] = 0\n",
    "data = pd.concat([train_df, test_df])\n",
    "print(train_df.shape)\n",
    "data.drop(columns = ['loan_default'], inplace = True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AVERAGE.ACCT.AGE</th>\n",
       "      <th>Aadhar_flag</th>\n",
       "      <th>Age_at_disbursaldate</th>\n",
       "      <th>CREDIT.HISTORY.LENGTH</th>\n",
       "      <th>Converted_AVERAGE.ACCT.AGE</th>\n",
       "      <th>Converted_CREDIT.HISTORY.LENGTH</th>\n",
       "      <th>DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS</th>\n",
       "      <th>Date.of.Birth</th>\n",
       "      <th>DisbursalDate</th>\n",
       "      <th>Disbursal_Day</th>\n",
       "      <th>...</th>\n",
       "      <th>manuf_id__49</th>\n",
       "      <th>manuf_id__51</th>\n",
       "      <th>manuf_id__67</th>\n",
       "      <th>manuf_id__86</th>\n",
       "      <th>manuf_id__120</th>\n",
       "      <th>manuf_id__145</th>\n",
       "      <th>manuf_id__152</th>\n",
       "      <th>manuf_id__153</th>\n",
       "      <th>manuf_id__155</th>\n",
       "      <th>manuf_id__156</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1</td>\n",
       "      <td>34.205479</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1984-01-01</td>\n",
       "      <td>2018-03-08</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1yrs 11mon</td>\n",
       "      <td>1</td>\n",
       "      <td>33.178082</td>\n",
       "      <td>1yrs 11mon</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1985-07-31</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1</td>\n",
       "      <td>32.397260</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1985-08-24</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0yrs 8mon</td>\n",
       "      <td>1</td>\n",
       "      <td>24.838356</td>\n",
       "      <td>1yrs 3mon</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1993-12-30</td>\n",
       "      <td>2018-10-26</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>1</td>\n",
       "      <td>41.065753</td>\n",
       "      <td>0yrs 0mon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1977-09-12</td>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10248 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  AVERAGE.ACCT.AGE  Aadhar_flag  Age_at_disbursaldate CREDIT.HISTORY.LENGTH  \\\n",
       "0        0yrs 0mon            1             34.205479             0yrs 0mon   \n",
       "1       1yrs 11mon            1             33.178082            1yrs 11mon   \n",
       "2        0yrs 0mon            1             32.397260             0yrs 0mon   \n",
       "3        0yrs 8mon            1             24.838356             1yrs 3mon   \n",
       "4        0yrs 0mon            1             41.065753             0yrs 0mon   \n",
       "\n",
       "   Converted_AVERAGE.ACCT.AGE  Converted_CREDIT.HISTORY.LENGTH  \\\n",
       "0                           0                                0   \n",
       "1                          13                               13   \n",
       "2                           0                                0   \n",
       "3                           8                               15   \n",
       "4                           0                                0   \n",
       "\n",
       "   DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS Date.of.Birth DisbursalDate  \\\n",
       "0                                    0    1984-01-01    2018-03-08   \n",
       "1                                    1    1985-07-31    2018-09-26   \n",
       "2                                    0    1985-08-24    2018-01-08   \n",
       "3                                    0    1993-12-30    2018-10-26   \n",
       "4                                    0    1977-09-12    2018-09-26   \n",
       "\n",
       "   Disbursal_Day      ...        manuf_id__49  manuf_id__51  manuf_id__67  \\\n",
       "0              8      ...                   0             0             0   \n",
       "1             26      ...                   0             0             0   \n",
       "2              8      ...                   0             0             0   \n",
       "3             26      ...                   0             0             0   \n",
       "4             26      ...                   0             0             0   \n",
       "\n",
       "  manuf_id__86  manuf_id__120  manuf_id__145  manuf_id__152  manuf_id__153  \\\n",
       "0            0              0              0              0              0   \n",
       "1            0              0              0              0              0   \n",
       "2            0              0              0              0              0   \n",
       "3            0              0              0              0              0   \n",
       "4            0              0              0              0              0   \n",
       "\n",
       "   manuf_id__155  manuf_id__156  \n",
       "0              0              0  \n",
       "1              0              0  \n",
       "2              0              0  \n",
       "3              0              0  \n",
       "4              0              0  \n",
       "\n",
       "[5 rows x 10248 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.get_dummies(data, prefix = 'pin_id_', columns= ['Current_pincode_ID'])\n",
    "data  = pd.get_dummies(data, prefix = 'supp_id_', columns= ['supplier_id'])\n",
    "data  = pd.get_dummies(data, prefix = 'manuf_id_', columns= ['manufacturer_id'])\n",
    "# data  = pd.get_dummies(data, prefix = 'st_id_', columns= ['State_ID'])\n",
    "# data  = pd.get_dummies(data, prefix = 'brnch_id_', columns= ['branch_id'])\n",
    "# data  = pd.get_dummies(data, prefix = 'emp_id_', columns= ['Employee_code_ID'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345546, 10248)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data,pin_code,supp_id , manf_id, st_id, brnch_id, emp_id])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_features = ['disbursed_amount', 'asset_cost', 'ltv', 'Aadhar_flag', 'PAN_flag', 'VoterID_flag',\\\n",
    "                'Driving_flag', 'Passport_flag', 'PERFORM_CNS.SCORE', 'PRI.NO.OF.ACCTS', 'PRI.ACTIVE.ACCTS', 'PRI.OVERDUE.ACCTS',\n",
    "                'PRI.CURRENT.BALANCE', 'PRI.SANCTIONED.AMOUNT', 'PRI.DISBURSED.AMOUNT', 'SEC.NO.OF.ACCTS',\n",
    "                'SEC.ACTIVE.ACCTS', 'SEC.OVERDUE.ACCTS', 'SEC.CURRENT.BALANCE',\n",
    "               'SEC.SANCTIONED.AMOUNT', 'SEC.DISBURSED.AMOUNT', 'PRIMARY.INSTAL.AMT',\n",
    "               'SEC.INSTAL.AMT', 'NEW.ACCTS.IN.LAST.SIX.MONTHS',\n",
    "               'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS', 'NO.OF_INQUIRIES',\n",
    "               'Age_at_disbursaldate',  'Converted_AVERAGE.ACCT.AGE',\n",
    "               'Converted_CREDIT.HISTORY.LENGTH', 'Employment_ind','Total_verification_flags','VoterId_Pan_flag_Combo',\n",
    "           'Disbursal_Month','Disbursal_Day','PERFORM_CNS.SCORE.DESCRIPTION']+ list(data.columns[51:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data.iloc[:233154,:]\n",
    "test_data = data.iloc[233154:,:]\n",
    "train_data['loan_default'] = train_df['loan_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del [data,train_df, test_df]\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112392, 10248)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-d7a1e94cc24c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0moof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfold_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrn_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_idx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loan_default'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fold n : {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfold_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mvalues\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4631\u001b[0m         \"\"\"\n\u001b[0;32m   4632\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_AXIS_REVERSED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4635\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mas_array\u001b[1;34m(self, transpose, items)\u001b[0m\n\u001b[0;32m   3947\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3949\u001b[1;33m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interleave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtranspose\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m_interleave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3958\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_interleaved_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3960\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Doing K fold crossvalidation\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = StratifiedKFold(n_splits= NFOLDS, shuffle=True, random_state=218)\n",
    "\n",
    "features2 = final_features\n",
    "\n",
    "categorical_columns = ['VoterId_Pan_flag_Combo','Disbursal_Month','Employment_ind']\n",
    "predictions4 = np.zeros(len(test_data))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "start_time= time.time()\n",
    "score = [0 for _ in range(folds.n_splits)]\n",
    "oof = np.zeros(len(train_data))\n",
    "max_iter = 5\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data.values, train_data['loan_default'].values)):\n",
    "    print(\"fold n : {}\".format(fold_))\n",
    "    \n",
    "    trn_data = lgb.Dataset(train_data.iloc[trn_idx][features2],\n",
    "                           label=train_data.iloc[trn_idx]['loan_default'], categorical_feature = categorical_columns\n",
    "                          )\n",
    "    val_data = lgb.Dataset(train_data.iloc[val_idx][features2],\n",
    "                           label=train_data.iloc[val_idx]['loan_default'] , categorical_feature = categorical_columns\n",
    "                          )\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param,\n",
    "                    trn_data,\n",
    "                    num_round,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds = 100)\n",
    "    oof[val_idx] = clf.predict(train_data.iloc[val_idx][features2], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features2\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    # we perform predictions by chunks\n",
    "    current_pred = clf.predict(test_data[features2].values)\n",
    "    predictions2 += current_pred / min(folds.n_splits, max_iter)\n",
    "   \n",
    "    print(\"time elapsed: {:<5.2}min\".format((time.time() - start_time) / 60))\n",
    "    score[fold_] = roc_auc_score(train_data.iloc[val_idx]['loan_default'].values, oof[val_idx])\n",
    "    if fold_ == max_iter - 1: break\n",
    "        \n",
    "if (folds.n_splits == max_iter):\n",
    "    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(train_data['loan_default'].values, oof)))\n",
    "else:\n",
    "     print(\"CV score: {:<8.5f}\".format(sum(score) / max_iter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_dummy.csv')\n",
    "test_data.to_csv('test_dummy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Himanshu'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
